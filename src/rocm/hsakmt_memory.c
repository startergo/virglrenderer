/*
 * Copyright 2025 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 */

#include <sys/mman.h>

#include "hsakmt_memory.h"
#include "util/hsakmt_util.h"
#include "hsakmt_context.h"
#include "hsakmt_vm.h"
#include <hsakmt/hsakmttypes.h>

inline bool
vhsakmt_check_va_valid(UNUSED struct vhsakmt_context *ctx, UNUSED uint64_t value)
{
#ifdef VHSA_CHECK_VA_ENABLE
   if (!VHSA_VAMGR.vm_va_base_addr || !VHSA_VAMGR.vm_va_high_addr)
      return false;

   if (value >= VHSA_VAMGR.vm_va_base_addr &&
       value < VHSA_VAMGR.vm_va_high_addr)
      return true;

   return false;
#else
   return true;
#endif
}

int
vhsakmt_gpu_unmap(struct vhsakmt_object *obj)
{
   return hsaKmtUnmapMemoryToGPU(obj->bo);
}

int
vhsakmt_free_userptr(UNUSED struct vhsakmt_object *obj)
{
   if (!obj || obj->type != VHSAKMT_OBJ_USERPTR)
      return -EINVAL;

   if (obj->iov && obj->iov_count && obj->bo) {
#ifdef VHSA_REGISTER_RANGES
      hsaKmtUnmapMemoryToGPU(obj->bo);
      hsaKmtDeregisterMemory(obj->bo);
#else
      munmap(obj->bo, obj->base.size);
#endif
   }

   return 0;
}

int
vhsakmt_free_scratch_map_mem(struct vhsakmt_context *ctx, struct vhsakmt_object *obj)
{
   if (!obj || obj->type != VHSAKMT_OBJ_SCRATCH_MAP_MEM)
      return -EINVAL;

   vhsa_dbg("free scratch memory %p, size 0x%lx", obj->bo, obj->base.size);

   return vhsakmt_gpu_unmap(obj);
}

int
vhsakmt_free_scratch_reserve_mem(struct vhsakmt_context *ctx, struct vhsakmt_object *obj)
{
   uint32_t i;

   for (i = 0; i < vhsakmt_device_backend()->vhsakmt_num_nodes; i++) {
      struct vhsakmt_node *node = &vhsakmt_device_backend()->vhsakmt_nodes[i];
      if (obj->bo >= (void *)node->scratch_vamgr.vm_va_base_addr &&
          obj->bo < (void *)node->scratch_vamgr.vm_va_high_addr) {
         vhsa_dbg("free scratch reserve memory node %d, addr %p, size 0x%lx",
                  i, obj->bo, obj->base.size);
         hsakmt_free_from_vamgr(&node->scratch_vamgr, (uint64_t)obj->bo);
         return 0;
      }
   }

   vhsa_err("failed to find matching node for scratch memory %p", obj->bo);
   return -EINVAL;
}

static inline bool
vhsakmt_is_scratch_obj(struct vhsakmt_object *obj)
{
   if (!obj)
      return false;

   return ((HsaMemFlags *)&obj->flags)->ui32.Scratch;
}

static inline bool
vhsakmt_is_no_address_obj(struct vhsakmt_object *obj)
{
   if (!obj)
      return false;

   return ((HsaMemFlags *)&obj->flags)->ui32.NoAddress;
}

static bool
vhsakmt_queue_mem_obj_can_remove(struct vhsakmt_object *obj)
{
   if (obj->type != VHSAKMT_OBJ_QUEUE_MEM)
      return false;

   return (obj->queue_obj == NULL) && obj->guest_removed;
}

int
vhsakmt_free_host_mem(struct vhsakmt_context *ctx, struct vhsakmt_object *obj)
{
   if (!obj || (obj->type != VHSAKMT_OBJ_HOST_MEM && obj->type != VHSAKMT_OBJ_QUEUE_MEM))
      return -EINVAL;

   if (obj->type == VHSAKMT_OBJ_QUEUE_MEM) {
      if (!vhsakmt_queue_mem_obj_can_remove(obj)) {
         vhsa_dbg("queue mem obj remove skipped, res_id %d, addr %p",
                  obj->base.res_id, obj->bo);
         return -EBUSY;
      }
   }

   if (obj->fd && obj->base.blob_id == 0) {
      if (obj->bo) {
         munmap(obj->bo, obj->base.size);
         obj->bo = NULL;
         obj->base.size = 0;
      }

      close(obj->fd);
      obj->fd = -1;
      return 0;
   }

   if (vhsakmt_is_scratch_obj(obj))
      vhsakmt_free_scratch_reserve_mem(ctx, obj);
   else {
      vhsakmt_gpu_unmap(obj);
      hsaKmtFreeMemory(obj->bo, obj->base.size);

      if (!vhsakmt_is_no_address_obj(obj)) {
         vhsakmt_reserve_va((uint64_t)obj->bo, obj->base.size);
         if (hsakmt_free_from_vamgr(&VHSA_VAMGR, (uint64_t)obj->bo)) {
            vhsa_err("failed to free memory from vamgr, address %p", obj->bo);
            return -EINVAL;
         }
      }
   }

   if (obj->import_handle) {
      vhsa_dbg("freeing imported amdgpu bo handle %p, res_id %d",
               (void *)obj->import_handle, obj->base.res_id);
      amdgpu_bo_free(obj->import_handle);
      obj->import_handle = 0;
   }

   return 0;
}

void
vhsakmt_free_dmabuf_obj(UNUSED struct vhsakmt_context *ctx, struct vhsakmt_object *obj)
{
   if (!obj || obj->type != VHSAKMT_OBJ_DMA_BUF)
      return;

   if (obj->fd == -1)
      return;

   close(obj->fd);
   obj->fd = -1;
}

static int
vhsakmt_scratch_init(struct vhsakmt_context *ctx, struct vhsakmt_node *node,
                     struct vhsakmt_ccmd_memory_req *req, struct vhsakmt_backend *b)
{
   int ret = 0;
   void *mem;

   pthread_mutex_lock(&b->hsakmt_mutex);
   if (node->scratch_base)
      goto out;

   mem = (void *)node->scratch_vamgr.vm_va_base_addr;

   vhsakmt_dereserve_va(node->scratch_vamgr.vm_va_base_addr, node->scratch_vamgr.reserve_size);

   ret = hsaKmtAllocMemory(req->alloc_args.PreferredNode,
                           node->scratch_vamgr.reserve_size,
                           req->alloc_args.MemFlags, &mem);
   if (ret) {
      vhsa_err("alloc scratch failed, ret %d (%s)", ret, strerror(errno));
      goto out;
   }
   if (node->scratch_vamgr.vm_va_base_addr != (uint64_t)mem) {
      vhsa_err("alloc scratch mismatch addr %p size 0x%lx, ret %d (%s)",
               mem, node->scratch_vamgr.reserve_size, ret, strerror(errno));
      hsaKmtFreeMemory(mem, node->scratch_vamgr.reserve_size);
      ret = -ENOMEM;
      goto out;
   }

   vhsa_dbg("scratch init %p, size 0x%lx",
            (void *)node->scratch_vamgr.vm_va_base_addr, node->scratch_vamgr.reserve_size);

   node->scratch_base = mem;

out:
   pthread_mutex_unlock(&b->hsakmt_mutex);
   vhsakmt_reserve_va(node->scratch_vamgr.vm_va_base_addr, node->scratch_vamgr.reserve_size);
   return ret;
}

static int
vhsakmt_alloc_scratch(struct vhsakmt_context *ctx,
                      struct vhsakmt_ccmd_memory_req *req, void **MemoryAddress)
{
   int ret = 0;
   void *mem;

   struct vhsakmt_node *node =
       vhsakmt_device_get_node(vhsakmt_device_backend(), req->alloc_args.PreferredNode);
   if (!node) {
      vhsa_err("invalid node %d", req->alloc_args.PreferredNode);
      return HSAKMT_STATUS_INVALID_NODE_UNIT;
   }

   if (!node->scratch_base) {
      ret = vhsakmt_scratch_init(ctx, node, req, vhsakmt_device_backend());
      if (ret)
         return ret;
   }

   mem = (void *)hsakmt_alloc_from_vamgr(&node->scratch_vamgr,
                                         req->alloc_args.SizeInBytes);
   if (!mem) {
      vhsa_err("cannot alloc scratch from vamgr, size 0x%lx",
               req->alloc_args.SizeInBytes);
      return -ENOMEM;
   }

   vhsa_dbg("scratch alloc node %d, addr %p, size 0x%lx",
            req->alloc_args.PreferredNode, mem, req->alloc_args.SizeInBytes);

   *MemoryAddress = mem;
   return 0;
}

static int
vhsakmt_alloc_align(struct vhsakmt_context *ctx,
                    struct vhsakmt_ccmd_memory_req *req, void **MemoryAddress, uint64_t align)
{
   int ret = 0;
   void *mem = NULL;

   if (align && (align & (align - 1)) != 0) {
      vhsa_err("align value 0x%lx is not power of 2", align);
      return -EINVAL;
   }

   if (align) {
      mem = (void *)hsakmt_alloc_from_vamgr_aligned(&VHSA_VAMGR,
                                                    req->alloc_args.SizeInBytes, align);
   } else {
      mem = (void *)hsakmt_alloc_from_vamgr(&VHSA_VAMGR, req->alloc_args.SizeInBytes);
   }

   if (!mem) {
      vhsa_err("cannot alloc from vamgr, size 0x%lx, vamgr usage: %.2f%%", req->alloc_args.SizeInBytes,
          ((double)VHSA_VAMGR.mem_used_size / (double)VHSA_VAMGR.reserve_size) * 100.0);
      return -ENOMEM;
   }

   *MemoryAddress = mem;

   vhsakmt_dereserve_va((uint64_t)mem, req->alloc_args.SizeInBytes);

   ret = hsaKmtAllocMemory(req->alloc_args.PreferredNode, req->alloc_args.SizeInBytes,
                           req->alloc_args.MemFlags, MemoryAddress);

   if (ret) {
      vhsa_err("alloc memory failed, target %p, size 0x%lx, ret %d (%s)",
               mem, req->alloc_args.SizeInBytes, ret, strerror(errno));
      goto failed_free_vamgr;
   }

   if (*MemoryAddress != mem) {
      vhsa_err("alloc memory mismatch: target %p != real %p", mem, *MemoryAddress);
      goto failed_free_mem;
   }

   return 0;

failed_free_mem:
   hsaKmtFreeMemory(*MemoryAddress, req->alloc_args.SizeInBytes);
failed_free_vamgr:
   hsakmt_free_from_vamgr(&VHSA_VAMGR, (uint64_t)mem);
   vhsakmt_reserve_va((uint64_t)mem, req->alloc_args.SizeInBytes);
   *MemoryAddress = NULL;
   return ret;
}

static int
vhsakmt_alloc_host(struct vhsakmt_context *ctx,
                   struct vhsakmt_ccmd_memory_req *req, void **MemoryAddress)
{
   return vhsakmt_alloc_align(ctx, req, MemoryAddress, req->alloc_args.Alignment);
}

static int
vhsakmt_alloc_device(struct vhsakmt_context *ctx,
                     struct vhsakmt_ccmd_memory_req *req, void **MemoryAddress)
{
   return vhsakmt_alloc_align(ctx, req, MemoryAddress, vhsakmt_page_size() << 9);
}

static int
vhsakmt_alloc_non_address(UNUSED struct vhsakmt_context *ctx,
                          struct vhsakmt_ccmd_memory_req *req, void **MemoryAddress)
{
   return hsaKmtAllocMemory(req->alloc_args.PreferredNode,
                            req->alloc_args.SizeInBytes,
                            req->alloc_args.MemFlags, MemoryAddress);
}

static int
vhsakmt_merge_va_ranges_mremap(struct vhsakmt_context *ctx, void **va, struct iovec *iov,
                               size_t iov_count, size_t *size)
{
   void *contiguous_va = NULL;
   size_t total_size = 0;
   size_t offset = 0;
   int ret = 0;
   void *result;
   bool va_provided = (va && *va != NULL);

   if (!va || !iov || iov_count == 0 || !size)
      return -EINVAL;

   for (size_t i = 0; i < iov_count; i++)
      total_size += iov[i].iov_len;

   if (va_provided)
      contiguous_va = *va;
   else {
      contiguous_va = mmap(NULL, total_size, PROT_READ | PROT_WRITE,
                           MAP_SHARED | MAP_ANONYMOUS, -1, 0);
      if (contiguous_va == MAP_FAILED) {
         vhsa_err("failed to allocate contiguous VA space, size 0x%lx: %s",
                  total_size, strerror(errno));
         return -ENOMEM;
      }
   }

   for (size_t i = 0; i < iov_count; i++) {
      result = mremap(iov[i].iov_base, iov[i].iov_len, iov[i].iov_len,
                     MREMAP_MAYMOVE | MREMAP_FIXED | MREMAP_DONTUNMAP, (char *)contiguous_va + offset);
      
      if (result == MAP_FAILED) {
         vhsa_err("mremap failed for region %zu (src=%p, dst=%p, size=0x%lx): %s",
                  i, iov[i].iov_base, (char *)contiguous_va + offset, iov[i].iov_len, strerror(errno));
         if (!va_provided)
            munmap(contiguous_va, total_size);
         ret = -EFAULT;
         goto cleanup;
      }
      
      offset += iov[i].iov_len;
   }

   *va = contiguous_va;
   *size = total_size;
   return 0;

cleanup:
   return ret;
}

static int
vhsakmt_alloc_memory(struct vhsakmt_context *ctx, struct vhsakmt_ccmd_memory_req *req,
                     void **MemoryAddress)
{
   int ret = 0;
   struct vhsakmt_object *obj;

   if (!vhsakmt_context_blob_id_valid(ctx, req->blob_id)) {
      vhsa_err("invalid blob_id %ld", req->blob_id);
      return -EINVAL;
   }

   req->alloc_args.MemFlags.ui32.FixedAddress = 1;

   if (req->alloc_args.MemFlags.ui32.Scratch)
      ret = vhsakmt_alloc_scratch(ctx, req, MemoryAddress);
   else if (req->alloc_args.MemFlags.ui32.NoAddress) {
      req->alloc_args.MemFlags.ui32.FixedAddress = 0;
      ret = vhsakmt_alloc_non_address(ctx, req, MemoryAddress);
   } else if (!req->alloc_args.PreferredNode ||
              !req->alloc_args.MemFlags.ui32.NonPaged ||
              req->alloc_args.MemFlags.ui32.GTTAccess ||
              req->alloc_args.MemFlags.ui32.OnlyAddress)
      ret = vhsakmt_alloc_host(ctx, req, MemoryAddress);
   else
      ret = vhsakmt_alloc_device(ctx, req, MemoryAddress);

   if (ret)
      return ret;

   obj = vhsakmt_context_object_create(*MemoryAddress, req->alloc_args.MemFlags.Value,
                               req->alloc_args.SizeInBytes, VHSAKMT_OBJ_HOST_MEM);

   vhsakmt_context_object_set_blob_id(ctx, obj, req->blob_id);

   return 0;
}

int
vhsakmt_ccmd_memory(struct vhsakmt_base_context *bctx, struct vhsakmt_ccmd_req *hdr)
{
   struct vhsakmt_ccmd_memory_req *req = to_vhsakmt_ccmd_memory_req(hdr);
   struct vhsakmt_context *ctx = to_vhsakmt_context(bctx);
   struct vhsakmt_ccmd_memory_rsp *rsp = NULL;
   unsigned rsp_len = sizeof(*rsp);

   VHSA_RSP_ALLOC(ctx, hdr, rsp_len);
   if (!rsp)
      return -ENOMEM;

   switch (req->type) {
   case VHSAKMT_CCMD_MEMORY_ALLOC: {
      void *MemoryAddress = NULL;
      rsp->ret = vhsakmt_alloc_memory(ctx, req, &MemoryAddress);
      rsp->memory_handle = (uint64_t)MemoryAddress;
      vhsa_dbg("alloc memory node %d, addr %p, size 0x%lx, flags 0x%x, ret %d", req->alloc_args.PreferredNode,
               MemoryAddress, req->alloc_args.SizeInBytes, req->alloc_args.MemFlags.Value, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_MAP_TO_GPU_NODES: {
      HSAuint64 AlternateVAGPU = 0;
      VHSA_CHECK_VA(req->map_to_GPU_nodes_args.MemoryAddress);
      rsp->ret = hsaKmtMapMemoryToGPUNodes(
          (void *)req->map_to_GPU_nodes_args.MemoryAddress,
          req->map_to_GPU_nodes_args.MemorySizeInBytes, &AlternateVAGPU,
          req->map_to_GPU_nodes_args.MemMapFlags,
          req->map_to_GPU_nodes_args.NumberOfNodes, (HSAuint32 *)req->payload);
      rsp->alternate_vagpu = AlternateVAGPU;
      break;
   }
   case VHSAKMT_CCMD_MEMORY_FREE: {
      VHSA_CHECK_VA(req->free_args.MemoryAddress);
      rsp->ret = hsaKmtFreeMemory((void *)req->free_args.MemoryAddress,
                                  req->free_args.SizeInBytes);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_UNMAP_TO_GPU: {
      VHSA_CHECK_VA(req->MemoryAddress);
      rsp->ret = hsaKmtUnmapMemoryToGPU((void *)req->MemoryAddress);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_AVAIL_MEM: {
      rsp->ret = hsaKmtAvailableMemory(req->Node, &rsp->available_bytes);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_MAP_MEM_TO_GPU: {
      HSAuint64 AlternateVAGPU = 0;
      VHSA_CHECK_VA(req->map_to_GPU_args.MemoryAddress);
      rsp->ret = hsaKmtMapMemoryToGPU((void *)req->map_to_GPU_args.MemoryAddress,
                                      req->map_to_GPU_args.MemorySizeInBytes, &AlternateVAGPU);
      if (rsp->ret)
         break;

      rsp->alternate_vagpu = AlternateVAGPU;
      if (req->map_to_GPU_args.need_create_bo) {
         struct vhsakmt_object *obj;
         obj = vhsakmt_context_object_create((void *)req->map_to_GPU_args.MemoryAddress, 0,
                                     req->map_to_GPU_args.MemorySizeInBytes,
                                     VHSAKMT_OBJ_SCRATCH_MAP_MEM);
         if (!obj) {
            vhsa_err("create scratch map object failed");
            rsp->ret = -ENOMEM;
            break;
         }
         vhsakmt_context_object_set_blob_id(ctx, obj, req->blob_id);
      }

      break;
   }
   case VHSAKMT_CCMD_MEMORY_REG_MEM_WITH_FLAG: {
      struct vhsakmt_object *obj = NULL;
      void *contiguous_va = NULL;
      size_t total_size = 0;
      VHSA_CHECK_VA(req->reg_mem_with_flag.MemoryAddress);

      obj = vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("register memory: object not found, res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      if (obj && obj->iov && obj->iov_count) {
#ifdef VHSA_REGISTER_RANGES
         munmap(obj->bo, obj->base.size);
         rsp->ret = hsaKmtRegisterMemoryRangesWithFlags(obj->bo, (HsaMemoryRange *)obj->iov, obj->iov_count,
                                                        req->reg_mem_with_flag.MemFlags);
#else
         contiguous_va = obj->bo;
         rsp->ret = vhsakmt_merge_va_ranges_mremap(ctx, &contiguous_va, obj->iov, obj->iov_count, &total_size);
         if (rsp->ret) {
             vhsa_err("failed to merge VA ranges: %d", rsp->ret);
             break;
         }

         rsp->ret = hsaKmtRegisterMemoryWithFlags(contiguous_va, total_size,
                                                  req->reg_mem_with_flag.MemFlags);

         if (rsp->ret == 0) {
            obj->bo = contiguous_va;
            obj->base.size = total_size;
         } else {
            vhsa_err("failed to register contiguous memory: %d", rsp->ret);
            if (contiguous_va != obj->bo)
               munmap(contiguous_va, total_size);
         }
#endif
      } else
         rsp->ret = hsaKmtRegisterMemoryWithFlags((void *)req->reg_mem_with_flag.MemoryAddress,
                                                  req->reg_mem_with_flag.MemorySizeInBytes,
                                                  req->reg_mem_with_flag.MemFlags);

      break;
   }
   case VHSAKMT_CCMD_MEMORY_DEREG_MEM: {
      VHSA_CHECK_VA(req->MemoryAddress);
      rsp->ret = hsaKmtDeregisterMemory((void *)req->MemoryAddress);
      if (req->res_id) {
         struct vhsakmt_object *obj =
             vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
         if (obj && (obj->type & VHSAKMT_OBJ_DMA_BUF))
            vhsakmt_context_free_object(&ctx->base, &obj->base);
      }

      break;
   }
   case VHSAKMT_CCMD_MEMORY_MAP_USERPTR: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (obj) {
         rsp->map_userptr_rsp.userptr_handle = (uint64_t)obj->bo;
         rsp->map_userptr_rsp.npfns = obj->base.size;
         rsp->ret = 0;
         break;
      }

      vhsa_err("cannot find userptr BO, invalid res_id %d", req->res_id);
      rsp->ret = -EINVAL;
      break;
   }
   case VHSAKMT_CCMD_MEMORY_EXPORT_DMABUF: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find BO for export dmabuf, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      if (obj->exported && obj->fd != -1) {
         rsp->export_dmabuf_rsp.offset = obj->export_offset;
         rsp->export_dmabuf_rsp.dmabuf_fd = obj->fd;
         rsp->ret = 0;
         break;
      }

      rsp->ret = hsaKmtExportDMABufHandle(
          obj->bo,
          req->export_dmabuf_args.MemorySizeInBytes,
          &obj->fd,
          &rsp->export_dmabuf_rsp.offset);
      
      if (!rsp->ret) {
         obj->export_offset = rsp->export_dmabuf_rsp.offset;
         obj->exported = true;
      }

      vhsa_dbg("ExportDMABuf: MemoryAddress=%p, MemorySizeInBytes=0x%lx, dmabuf_fd=%d, offset=0x%lx, ret=%d",
               (void *)req->export_dmabuf_args.MemoryAddress, req->export_dmabuf_args.MemorySizeInBytes,
               obj->fd, rsp->export_dmabuf_rsp.offset, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_AMDGPU_IMPORT: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find BO for amdgpu import, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      if (!obj->exported) {
         vhsa_err("BO dmabuf not exported yet, res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      rsp->ret = amdgpu_bo_import(
          (void *)req->amdgpu_import_args.dev,
          req->amdgpu_import_args.type,
          obj->fd,
          &rsp->amdgpu_import_rsp.output);

      if (!rsp->ret) {
         obj->import_handle = rsp->amdgpu_import_rsp.output.buf_handle;
         obj->import_size = rsp->amdgpu_import_rsp.output.alloc_size;
         obj->imported = true;
      }

      vhsa_dbg("AMDGPU Import: dev=0x%lx, type=%u, shared_handle=%u, bo_handle=%p, size=0x%lx, ret=%d",
               req->amdgpu_import_args.dev, req->amdgpu_import_args.type, obj->fd,
               (void *)rsp->amdgpu_import_rsp.output.buf_handle,
               rsp->amdgpu_import_rsp.output.alloc_size, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_AMDGPU_EXPORT: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find BO for amdgpu export, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      if (obj->import_handle == 0) {
         vhsa_err("amdgpu bo not imported yet, res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      rsp->ret = amdgpu_bo_export(
          obj->import_handle,
          req->amdgpu_export_args.type,
          &rsp->shared_handle);
      vhsa_dbg("AMDGPU Export: buf_handle=0x%lx, type=%u, shared_handle=%u, ret=%d",
               req->amdgpu_export_args.buf_handle, req->amdgpu_export_args.type,
               rsp->shared_handle, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_AMDGPU_VA_OP: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find BO for amdgpu va op, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }
      rsp->ret = amdgpu_bo_va_op(
          obj->import_handle,
          req->amdgpu_va_op_args.offset,
          req->amdgpu_va_op_args.size,
          req->amdgpu_va_op_args.addr,
          req->amdgpu_va_op_args.flags,
          req->amdgpu_va_op_args.ops);
      vhsa_dbg("AMDGPU VA OP: bo=%p, offset=0x%lx, size=0x%lx, addr=0x%lx, flags=0x%lx, ops=%u, ret=%d",
               (void *)obj->import_handle, req->amdgpu_va_op_args.offset,
               req->amdgpu_va_op_args.size, req->amdgpu_va_op_args.addr,
               req->amdgpu_va_op_args.flags, req->amdgpu_va_op_args.ops, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_AMDGPU_BO_FREE: {
      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find BO for amdgpu bo free, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      if (!obj->import_handle || !obj->imported) {
         vhsa_err("amdgpu bo not imported yet, res_id %d", req->res_id);
         rsp->ret = 0;
         break;
      }

      rsp->ret = amdgpu_bo_free((void *)obj->import_handle);
      obj->import_handle = 0;
      obj->imported = false;
      vhsa_dbg("AMDGPU BO Free: bo=0x%lx, ret=%d", req->buf_handle, rsp->ret);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_SHARE_MEMORY: {
      VHSA_CHECK_VA(req->share_memory_args.MemoryAddress);
      rsp->ret = hsaKmtShareMemory((void *)req->share_memory_args.MemoryAddress,
                                   req->share_memory_args.MemorySizeInBytes, (void*)&rsp->shared_handle);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_REGISTER_SHARED_HANDLE: {
      struct vhsakmt_object *obj;

      if (req->register_shared_handle_args.NumberOfNodes > VHSAKMT_MEMORY_MAX_NODES) {
         rsp->ret = HSAKMT_STATUS_INVALID_PARAMETER;
         break;
      }

      rsp->ret = hsaKmtRegisterSharedHandleToNodes(
         (void *)req->register_shared_handle_args.SharedMemoryHandle,
         (void **)&rsp->register_shared_handle_rsp.memory_handle, &rsp->register_shared_handle_rsp.size,
         req->register_shared_handle_args.NumberOfNodes, (HSAuint32 *)req->payload);

      if (rsp->ret)
         break;

      obj = vhsakmt_context_object_create((void *)rsp->register_shared_handle_rsp.memory_handle, 0,
                                          rsp->register_shared_handle_rsp.size, VHSAKMT_OBJ_HOST_MEM);
      if (!obj) {
         hsaKmtDeregisterMemory((void *)rsp->register_shared_handle_rsp.memory_handle);
         vhsa_err("create shared handle object failed");
         rsp->ret = -ENOMEM;
         break;
      }
      break;
   }
   case VHSAKMT_CCMD_MEMORY_SET_MEM_POLICY: {
      rsp->ret = hsaKmtSetMemoryPolicy(req->set_mem_policy_args.Node, req->set_mem_policy_args.DefaultPolicy,
                                       req->set_mem_policy_args.AlternatePolicy,
                                       (void *)req->set_mem_policy_args.MemoryAddressAlternate,
                                       req->set_mem_policy_args.MemorySizeInBytes);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_SVM_GET_ATTR: {
      VHSA_CHECK_VA(req->svm_attr_args.start_addr);
      struct vhsakmt_object *obj =
         vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find SVM BO, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      rsp->ret = hsaKmtSVMGetAttr(obj->bo, req->svm_attr_args.size, req->svm_attr_args.nattr,
                                 (HSA_SVM_ATTRIBUTE *)req->payload);
      break;
   }
   case VHSAKMT_CCMD_MEMORY_SVM_SET_ATTR: {
      VHSA_CHECK_VA(req->svm_attr_args.start_addr);
      struct vhsakmt_object *obj =
         vhsakmt_context_get_object_from_res_id(ctx, req->res_id);
      if (!obj) {
         vhsa_err("cannot find SVM BO, invalid res_id %d", req->res_id);
         rsp->ret = -EINVAL;
         break;
      }

      rsp->ret = hsaKmtSVMSetAttr(obj->bo, req->svm_attr_args.size, req->svm_attr_args.nattr,
                                 (HSA_SVM_ATTRIBUTE *)req->payload);
      break;
   }
   default:
      vhsa_err("unsupported memory CMD %d", req->type);
   }

   if (rsp->ret)
      vhsa_err("memory command failed, type %d, ret %d", req->type, rsp->ret);

   return 0;
}

int
vhsakmt_ccmd_gl_inter(struct vhsakmt_base_context *bctx, struct vhsakmt_ccmd_req *hdr)
{
   const struct vhsakmt_ccmd_gl_inter_req *req = to_vhsakmt_ccmd_gl_inter_req(hdr);
   struct vhsakmt_context *ctx = to_vhsakmt_context(bctx);
   struct vhsakmt_ccmd_gl_inter_rsp *rsp = NULL;
   size_t rsp_len = sizeof(*rsp);

   switch (req->type) {
   case VHSAKMT_CCMD_GL_REG_GHD_TO_NODES: {
      HsaGraphicsResourceInfo info;
      HSA_REGISTER_MEM_FLAGS flags;
      int ret = 0;

      struct vhsakmt_object *obj =
          vhsakmt_context_get_object_from_res_id(ctx, req->reg_ghd_to_nodes.res_handle);
      if (!obj || obj->fd == -1) {
         vhsa_err("GL interop invalid dmabuf, res_id %u",
                  req->reg_ghd_to_nodes.res_handle);
         VHSA_RSP_ALLOC(ctx, hdr, rsp_len);
         rsp->ret = HSAKMT_STATUS_INVALID_HANDLE;
         break;
      }

      flags.Value = req->reg_ghd_to_nodes.flag;
      ret = hsaKmtRegisterGraphicsHandleToNodesExt(obj->fd, &info, req->reg_ghd_to_nodes.NumberOfNodes,
                                                   (HSAuint32 *)req->payload, flags);

      if (ret) {
         vhsa_err("register graphics handle failed, fd %d, ret %d", obj->fd, ret);
         VHSA_RSP_ALLOC(ctx, hdr, rsp_len);
      } else {
         vhsa_dbg("register graphics handle fd %d, addr %p, size 0x%lx",
                  obj->fd, info.MemoryAddress, info.SizeInBytes);
         rsp = vhsakmt_context_rsp(ctx, hdr, rsp_len + info.MetadataSizeInBytes);
         memcpy(&rsp->info, &info, sizeof(info));
         memcpy(&rsp->payload, info.Metadata, info.MetadataSizeInBytes);
      }

      rsp->ret = ret;
      break;
   }

   default:
      vhsa_err("unsupported GL interop command %d", req->type);
      break;
   }

   if (rsp && rsp->ret)
      vhsa_err("GL interop failed, type %d, ret %d", req->type, rsp->ret);

   return 0;
}
